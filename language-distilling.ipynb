{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:58:45.231306Z",
     "iopub.status.busy": "2025-05-18T16:58:45.231076Z",
     "iopub.status.idle": "2025-05-18T16:58:47.752914Z",
     "shell.execute_reply": "2025-05-18T16:58:47.752066Z",
     "shell.execute_reply.started": "2025-05-18T16:58:45.231282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ziadtarek12/language_distilling\n",
    "%cd language_distilling\n",
    "!git checkout eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:58:47.753978Z",
     "iopub.status.busy": "2025-05-18T16:58:47.753731Z",
     "iopub.status.idle": "2025-05-18T17:04:09.839999Z",
     "shell.execute_reply": "2025-05-18T17:04:09.839027Z",
     "shell.execute_reply.started": "2025-05-18T16:58:47.753942Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip uninstall -y torch torchvision torchaudio\n",
    "!pip install transformers==4.26.0\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install cytoolz\n",
    "!pip install tqdm\n",
    "!pip install torchtext==0.16.0\n",
    "!pip install torchvision==0.16.0\n",
    "!pip install torch==2.1.0\n",
    "!pip install torchaudio==2.1.0\n",
    "!pip install configargparse\n",
    "!pip install tensorboardX\n",
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T17:04:09.842579Z",
     "iopub.status.busy": "2025-05-18T17:04:09.841939Z",
     "iopub.status.idle": "2025-05-18T17:04:41.696868Z",
     "shell.execute_reply": "2025-05-18T17:04:41.696347Z",
     "shell.execute_reply.started": "2025-05-18T17:04:09.842552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import shelve\n",
    "import io\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T17:04:41.698022Z",
     "iopub.status.busy": "2025-05-18T17:04:41.697514Z",
     "iopub.status.idle": "2025-05-18T17:04:41.760736Z",
     "shell.execute_reply": "2025-05-18T17:04:41.760217Z",
     "shell.execute_reply.started": "2025-05-18T17:04:41.697988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('.')\n",
    "sys.path.append('./opennmt')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T17:04:41.761635Z",
     "iopub.status.busy": "2025-05-18T17:04:41.761432Z",
     "iopub.status.idle": "2025-05-18T17:05:09.024450Z",
     "shell.execute_reply": "2025-05-18T17:05:09.023115Z",
     "shell.execute_reply.started": "2025-05-18T17:04:41.761617Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create directories for data and outputs\n",
    "!mkdir -p data/\n",
    "!mkdir -p output/cmlm_model\n",
    "!mkdir -p output/bert_dump\n",
    "!mkdir -p output/kd-model/ckpt\n",
    "!mkdir -p output/kd-model/log\n",
    "!mkdir -p output/translation\n",
    "\n",
    "# Download IWSLT German-English dataset using the provided script\n",
    "!bash scripts/download-iwslt_deen.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T17:05:09.026231Z",
     "iopub.status.busy": "2025-05-18T17:05:09.025883Z",
     "iopub.status.idle": "2025-05-18T17:06:45.376184Z",
     "shell.execute_reply": "2025-05-18T17:06:45.375155Z",
     "shell.execute_reply.started": "2025-05-18T17:05:09.026184Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scripts.bert_tokenize import tokenize, process\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_model = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case='uncased' in bert_model)\n",
    "\n",
    "# Define data directories\n",
    "data_dir = \"data/de-en\"\n",
    "\n",
    "# BERT tokenize our dataset files\n",
    "for language in ['de', 'en']:\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        input_file = f\"{data_dir}/{split}.{language}\"\n",
    "        output_file = f\"{data_dir}/{split}.{language}.bert\"\n",
    "        print(f\"Tokenizing {input_file}...\")\n",
    "        \n",
    "        with open(input_file, 'r') as reader, open(output_file, 'w') as writer:\n",
    "            process(reader, writer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T17:06:45.377661Z",
     "iopub.status.busy": "2025-05-18T17:06:45.377330Z",
     "iopub.status.idle": "2025-05-18T17:07:27.083839Z",
     "shell.execute_reply": "2025-05-18T17:07:27.083069Z",
     "shell.execute_reply.started": "2025-05-18T17:06:45.377632Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dataset DB for BERT training\n",
    "from scripts.bert_prepro import main as bert_prepro\n",
    "\n",
    "# Set up args for bert_prepro\n",
    "prepro_args = argparse.Namespace(\n",
    "    src=f\"{data_dir}/train.de.bert\",\n",
    "    tgt=f\"{data_dir}/train.en.bert\",\n",
    "    output='data/DEEN.db'\n",
    ")\n",
    "\n",
    "# Run preprocessing\n",
    "bert_prepro(prepro_args)\n",
    "\n",
    "# Create vocabulary file using OpenNMT's preprocess.py\n",
    "print(\"Creating vocabulary files with OpenNMT preprocess.py...\")\n",
    "!python opennmt/preprocess.py \\\n",
    "    -train_src {data_dir}/train.de.bert \\\n",
    "    -train_tgt {data_dir}/train.en.bert \\\n",
    "    -valid_src {data_dir}/valid.de.bert \\\n",
    "    -valid_tgt {data_dir}/valid.en.bert \\\n",
    "    -save_data data/DEEN \\\n",
    "    -src_seq_length 150 -tgt_seq_length 150\n",
    "\n",
    "vocab_file = \"data/DEEN.vocab.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T17:45:30.857132Z",
     "iopub.status.busy": "2025-05-18T17:45:30.856374Z",
     "iopub.status.idle": "2025-05-18T17:45:35.701626Z",
     "shell.execute_reply": "2025-05-18T17:45:35.700985Z",
     "shell.execute_reply.started": "2025-05-18T17:45:30.857086Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Import needed modules\n",
    "from cmlm.data import BertDataset, TokenBucketSampler\n",
    "from cmlm.model import convert_embedding, BertForSeq2seq\n",
    "from cmlm.util import Logger, RunningMeter\n",
    "from run_cmlm_finetuning import noam_schedule\n",
    "\n",
    "# Load vocabulary using our compatibility module\n",
    "from vocab_loader import safe_load_vocab\n",
    "\n",
    "vocab_file = \"data/DEEN.vocab.pt\"\n",
    "train_file = \"data/DEEN.db\"\n",
    "valid_src = f\"{data_dir}/valid.de.bert\"\n",
    "valid_tgt = f\"{data_dir}/valid.en.bert\"\n",
    "output_dir = \"output/cmlm_model\"\n",
    "\n",
    "# Load vocabulary using custom loader to avoid PyTorch compatibility issues\n",
    "vocab_dump = safe_load_vocab(vocab_file)\n",
    "vocab = vocab_dump['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = BertDataset(train_file, tokenizer, vocab, seq_len=512, max_len=150)\n",
    "\n",
    "# Define sampler and data loader\n",
    "BUCKET_SIZE = 8192\n",
    "train_sampler = TokenBucketSampler(\n",
    "    train_dataset.lens, BUCKET_SIZE, 6144, batch_multiple=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=BertDataset.pad_collate)\n",
    "\n",
    "# Prepare model\n",
    "model = BertForSeq2seq.from_pretrained(bert_model)\n",
    "bert_embedding = model.bert.embeddings.word_embeddings.weight\n",
    "\n",
    "# Print model information before modifications\n",
    "hidden_size = model.config.hidden_size\n",
    "print(f\"Original model: BERT hidden size = {hidden_size}\")\n",
    "print(f\"Original model: BERT vocab size = {bert_embedding.size(0)}\")\n",
    "print(f\"Target vocabulary size = {len(vocab)}\")\n",
    "\n",
    "# Convert vocabulary to embedding form\n",
    "embedding = convert_embedding(tokenizer, vocab, bert_embedding)\n",
    "\n",
    "# Update model architecture to accommodate the new vocabulary size\n",
    "print(f\"Updating model architecture for vocabulary size: {embedding.size(0)}\")\n",
    "# Create a new decoder with correct dimensions\n",
    "model.cls.predictions.decoder = torch.nn.Linear(hidden_size, embedding.size(0), bias=True)\n",
    "model.cls.predictions.bias = torch.nn.Parameter(torch.zeros(embedding.size(0)))\n",
    "model.config.vocab_size = embedding.size(0)\n",
    "\n",
    "# Update the weights\n",
    "model.cls.predictions.decoder.weight.data.copy_(embedding.data)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "print(f\"Model adapted with vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-05-18T17:45:35.703233Z",
     "iopub.status.busy": "2025-05-18T17:45:35.702989Z",
     "iopub.status.idle": "2025-05-18T18:27:01.555418Z",
     "shell.execute_reply": "2025-05-18T18:27:01.553478Z",
     "shell.execute_reply.started": "2025-05-18T17:45:35.703216Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1  # Using proportion instead of absolute steps\n",
    "max_steps = 100000  # Full training uses 100k steps\n",
    "num_steps_to_run = 100000  # We'll do fewer steps for demonstration\n",
    "\n",
    "# Optimizer using modern AdamW from transformers\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(max_steps * warmup_proportion),\n",
    "    num_training_steps=max_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "running_loss = RunningMeter('loss')\n",
    "model.train()\n",
    "\n",
    "print(\"Starting CMLM fine-tuning...\")\n",
    "#Use a plain iterator instead of tqdm with len()\n",
    "train_iter = iter(train_loader)\n",
    "for step in range(num_steps_to_run):\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        # Restart iterator if we run out of batches\n",
    "        train_iter = iter(train_loader)\n",
    "        batch = next(train_iter)\n",
    "        \n",
    "    # Move batch to device\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, lm_label_ids = batch\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Create output mask from lm_label_ids for model forward pass\n",
    "    output_mask = lm_label_ids != -1  # Masking for non-padded tokens\n",
    "    \n",
    "    # Forward pass with output_mask parameter\n",
    "    loss = model(input_ids, segment_ids, input_mask, lm_label_ids, output_mask)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    running_loss(loss.item())\n",
    "    print(f\"Step {step}, Loss: {running_loss.val:.4f}\")\n",
    "    if step % 100 == 0:\n",
    "        \n",
    "        # Clear CUDA cache periodically to avoid memory issues\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), f\"{output_dir}/model_step_{num_steps_to_run}.pt\")\n",
    "print(f\"Model saved to {output_dir}/model_step_{num_steps_to_run}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.555958Z",
     "iopub.status.idle": "2025-05-18T18:27:01.556216Z",
     "shell.execute_reply": "2025-05-18T18:27:01.556106Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.556094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.557242Z",
     "iopub.status.idle": "2025-05-18T18:27:01.557582Z",
     "shell.execute_reply": "2025-05-18T18:27:01.557427Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.557412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import extraction functions\n",
    "from dump_teacher_hiddens import tensor_dumps, gather_hiddens, BertSampleDataset, batch_features, process_batch\n",
    "\n",
    "# Path to model checkpoint from Stage 1\n",
    "ckpt_path = f\"{output_dir}/model_step_{num_steps_to_run}.pt\"\n",
    "bert_dump_path = \"output/bert_dump\"\n",
    "\n",
    "# Load the fine-tuned BERT model\n",
    "state_dict = torch.load(ckpt_path)\n",
    "vsize = state_dict['cls.predictions.decoder.weight'].size(0)\n",
    "bert = BertForSeq2seq.from_pretrained(bert_model).eval()\n",
    "bert.to(device)\n",
    "\n",
    "# Fix: Instead of using update_output_layer_by_size, which pads to multiples of 8,\n",
    "# we'll directly resize the model layers to match the exact dimensions from the checkpoint\n",
    "print(f\"Resizing model to exact vocabulary size: {vsize}\")\n",
    "hidden_size = bert.config.hidden_size\n",
    "\n",
    "# Create exact-sized layers without padding to multiples of 8\n",
    "bert.cls.predictions.decoder = torch.nn.Linear(hidden_size, vsize, bias=True)\n",
    "bert.cls.predictions.bias = bert.cls.predictions.decoder.bias\n",
    "bert.config.vocab_size = vsize\n",
    "\n",
    "# Now load the state dict - should have matching dimensions\n",
    "bert.load_state_dict(state_dict)\n",
    "\n",
    "# Save the final projection layer\n",
    "linear = torch.nn.Linear(bert.config.hidden_size, bert.config.vocab_size)\n",
    "linear.weight.data = state_dict['cls.predictions.decoder.weight']\n",
    "linear.bias.data = state_dict['cls.predictions.bias']\n",
    "torch.save(linear, f'{bert_dump_path}/linear.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.558232Z",
     "iopub.status.idle": "2025-05-18T18:27:01.558489Z",
     "shell.execute_reply": "2025-05-18T18:27:01.558393Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.558383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to extract hidden states - with debugging option\n",
    "def build_db_batched(corpus_path, out_db, bert, toker, batch_size=8, debug_mode=False, max_samples=100):\n",
    "    dataset = BertSampleDataset(corpus_path, toker)\n",
    "    \n",
    "    # For debugging, limit the number of samples\n",
    "    if debug_mode:\n",
    "        # Get a subset of the dataset IDs\n",
    "        all_ids = dataset.ids\n",
    "        subset_ids = all_ids[:max_samples] if len(all_ids) > max_samples else all_ids\n",
    "        dataset.ids = subset_ids\n",
    "        print(f\"DEBUG MODE: Processing only {len(subset_ids)} samples instead of {len(all_ids)}\")\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                       num_workers=4, collate_fn=batch_features)\n",
    "    \n",
    "    with tqdm(desc='Computing BERT features', total=len(dataset)) as pbar:\n",
    "        for ids, *batch in loader:\n",
    "            outputs = process_batch(batch, bert, toker)\n",
    "            for id_, output in zip(ids, outputs):\n",
    "                if output is not None:\n",
    "                    out_db[id_] = tensor_dumps(output)\n",
    "            pbar.update(len(ids))\n",
    "            \n",
    "            # For debugging, break after the first batch if needed\n",
    "            if debug_mode and batch_size >= max_samples:\n",
    "                print(\"First batch processed, breaking early due to debug mode\")\n",
    "                break\n",
    "\n",
    "# Extract hidden states\n",
    "db_path = \"data/DEEN.db\"\n",
    "print(\"Extracting hidden states...\")\n",
    "\n",
    "# Set debug mode to True for faster debugging, False for full processing\n",
    "debug_mode = True  # Toggle this for quick debugging\n",
    "max_samples = 100  # Number of samples to process in debug mode\n",
    "\n",
    "with shelve.open(f'{bert_dump_path}/db', 'c') as out_db, torch.no_grad():\n",
    "    build_db_batched(db_path, out_db, bert, tokenizer, batch_size=8, \n",
    "                    debug_mode=debug_mode, max_samples=max_samples)\n",
    "\n",
    "# Free up GPU memory after extraction\n",
    "print(\"Clearing GPU memory...\")\n",
    "bert.cpu()  # Move model to CPU\n",
    "del bert    # Delete the model\n",
    "linear.cpu()  # Move linear layer to CPU\n",
    "torch.cuda.empty_cache()  # Empty the CUDA cache\n",
    "print(\"GPU memory cleared after hidden states extraction\")\n",
    "\n",
    "if debug_mode:\n",
    "    print(f\"DEBUG MODE: Hidden states for {max_samples} samples extracted to {bert_dump_path}/db\")\n",
    "    print(\"To run full extraction, set debug_mode=False\")\n",
    "else:\n",
    "    print(f\"Hidden states extracted and saved to {bert_dump_path}/db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.559537Z",
     "iopub.status.idle": "2025-05-18T18:27:01.559809Z",
     "shell.execute_reply": "2025-05-18T18:27:01.559703Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.559692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import functions for top-k computation\n",
    "from dump_teacher_topk import tensor_loads, dump_topk\n",
    "\n",
    "# Top-K parameter\n",
    "k = 8  # Following the paper\n",
    "\n",
    "# Load linear layer\n",
    "linear = torch.load(f'{bert_dump_path}/linear.pt')\n",
    "# Ensure the linear layer uses the same precision as the hidden states (FP16/Half)\n",
    "linear = linear.half()\n",
    "linear.to(device)\n",
    "\n",
    "# Compute top-k logits\n",
    "print(\"Computing top-k logits...\")\n",
    "with shelve.open(f'{bert_dump_path}/db', 'r') as db, \\\n",
    "     shelve.open(f'{bert_dump_path}/topk', 'c') as topk_db:\n",
    "    for key, value in tqdm(db.items(), total=len(db), desc='Computing topk...'):\n",
    "        bert_hidden = torch.tensor(tensor_loads(value)).to(device)\n",
    "        # bert_hidden is already in half precision, no need to convert\n",
    "        topk = linear(bert_hidden).topk(dim=-1, k=k)\n",
    "        dump = dump_topk(topk)\n",
    "        topk_db[key] = dump\n",
    "        \n",
    "        # Clear tensor from GPU memory after each iteration\n",
    "        del bert_hidden\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Final memory cleanup\n",
    "print(\"Clearing GPU memory...\")\n",
    "linear.cpu()  # Move linear layer to CPU\n",
    "del linear     # Delete the linear layer\n",
    "torch.cuda.empty_cache()  # Empty the CUDA cache\n",
    "print(\"GPU memory cleared after top-k computation\")\n",
    "print(f\"Top-k logits computed and saved to {bert_dump_path}/topk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.560742Z",
     "iopub.status.idle": "2025-05-18T18:27:01.560994Z",
     "shell.execute_reply": "2025-05-18T18:27:01.560878Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.560868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required modules for training\n",
    "from onmt.inputters.bert_kd_dataset import BertKdDataset, TokenBucketSampler\n",
    "from onmt.utils.optimizers import Optimizer\n",
    "from onmt.train_single import build_model_saver, build_trainer, cycle_loader\n",
    "import torch.nn as nn  # Add missing import\n",
    "import os  # Add import for checking file existence\n",
    "\n",
    "# Define paths\n",
    "data_db = \"data/DEEN.db\"\n",
    "bert_dump = \"output/bert_dump\"\n",
    "data = \"data/DEEN\"\n",
    "config_path = \"opennmt/config/config-transformer-base-mt-deen.yml\"\n",
    "output_path = \"output/kd-model\"\n",
    "\n",
    "# Check if required files exist and provide guidance\n",
    "print(\"Checking for required database files...\")\n",
    "topk_db_file = f\"{bert_dump}/topk\"\n",
    "topk_db_dir = os.path.dirname(topk_db_file)\n",
    "\n",
    "# First make sure the directory exists\n",
    "if not os.path.exists(topk_db_dir):\n",
    "    print(f\"Creating directory: {topk_db_dir}\")\n",
    "    os.makedirs(topk_db_dir, exist_ok=True)\n",
    "\n",
    "# Check if topk database exists\n",
    "if not any(os.path.exists(f\"{topk_db_file}{ext}\") for ext in [\"\", \".db\", \".dat\", \".bak\", \".dir\"]):\n",
    "    print(f\"Warning: Top-k database not found at {topk_db_file}\")\n",
    "    print(\"Running top-k computation from Stage 2...\")\n",
    "    \n",
    "    # Import functions for top-k computation if they haven't been imported yet\n",
    "    from dump_teacher_topk import tensor_loads, dump_topk\n",
    "    \n",
    "    # Load the fine-tuned BERT model if not already loaded\n",
    "    if 'linear' not in locals():\n",
    "        linear_path = f'{bert_dump}/linear.pt'\n",
    "        if os.path.exists(linear_path):\n",
    "            print(f\"Loading linear layer from {linear_path}\")\n",
    "            linear = torch.load(linear_path)\n",
    "            linear.to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Linear layer not found at {linear_path}. Please run Stage 2 first.\")\n",
    "    \n",
    "    # Check if hidden states database exists\n",
    "    db_path = f\"{bert_dump}/db\"\n",
    "    if not any(os.path.exists(f\"{db_path}{ext}\") for ext in [\"\", \".db\", \".dat\", \".bak\", \".dir\"]):\n",
    "        raise ValueError(f\"Hidden states database not found at {db_path}. Please run Stage 2 first.\")\n",
    "    \n",
    "    print(\"Computing top-k logits...\")\n",
    "    # Set k value for top-k computation\n",
    "    k = 8  # Following the paper\n",
    "    \n",
    "    # Create the topk database in create mode\n",
    "    with shelve.open(f'{bert_dump}/db', 'r') as db, \\\n",
    "         shelve.open(f'{bert_dump}/topk', 'c') as topk_db:\n",
    "        for key, value in tqdm(db.items(), total=len(db), desc='Computing topk...'):\n",
    "            # Load the hidden states and convert to the same data type as the linear layer\n",
    "            bert_hidden = torch.tensor(tensor_loads(value), dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Ensure same precision between hidden states and linear layer\n",
    "            if linear.weight.dtype != bert_hidden.dtype:\n",
    "                print(f\"Converting tensors to match dtypes - hidden: {bert_hidden.dtype}, linear: {linear.weight.dtype}\")\n",
    "                # Either convert hidden to match linear\n",
    "                if hasattr(linear, 'half') and linear.weight.dtype == torch.float16:\n",
    "                    bert_hidden = bert_hidden.half()\n",
    "                # Or convert linear to match hidden\n",
    "                else:\n",
    "                    linear = linear.float()\n",
    "                    \n",
    "            # Compute top-k\n",
    "            topk = linear(bert_hidden).topk(dim=-1, k=k)\n",
    "            dump = dump_topk(topk)\n",
    "            topk_db[key] = dump\n",
    "    \n",
    "    print(f\"Top-k logits computed and saved to {bert_dump}/topk\")\n",
    "else:\n",
    "    print(f\"Top-k database exists at {topk_db_file}\")\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "# Create args object\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "# Setup KD parameters\n",
    "args.train_from = None\n",
    "args.max_grad_norm = None\n",
    "args.kd_topk = 8\n",
    "args.train_steps = 100000\n",
    "args.kd_temperature = 10.0\n",
    "args.kd_alpha = 0.5\n",
    "args.warmup_steps = 8000\n",
    "args.learning_rate = 2.0\n",
    "args.bert_dump = bert_dump\n",
    "args.data_db = data_db\n",
    "args.bert_kd = True\n",
    "args.data = data\n",
    "\n",
    "# Add missing required parameters\n",
    "args.model_type = \"text\"  # Required for OpenNMT model builder\n",
    "args.copy_attn = False    # Common OpenNMT parameter\n",
    "args.global_attention = \"general\"  # Common OpenNMT parameter\n",
    "\n",
    "# Add embeddings parameters\n",
    "# If word_vec_size is already defined, use it for both src and tgt\n",
    "args.src_word_vec_size = args.word_vec_size\n",
    "args.tgt_word_vec_size = args.word_vec_size\n",
    "# Add any other required embedding parameters\n",
    "args.feat_merge = \"concat\"\n",
    "args.feat_vec_size = -1\n",
    "args.feat_vec_exponent = 0.7\n",
    "\n",
    "# Add pretrained word vectors parameters\n",
    "args.pre_word_vecs_enc = None  # Path to pretrained word vectors for encoder\n",
    "args.pre_word_vecs_dec = None  # Path to pretrained word vectors for decoder\n",
    "args.pre_word_vecs = None      # General pretrained word vectors\n",
    "\n",
    "# Add fix_word_vecs parameters that were missing\n",
    "args.fix_word_vecs_enc = False\n",
    "args.fix_word_vecs_dec = False\n",
    "\n",
    "# Add critical RNN and transformer parameters\n",
    "args.enc_rnn_size = args.rnn_size  # This was missing\n",
    "args.dec_rnn_size = args.rnn_size\n",
    "# Additional transformer-specific parameters\n",
    "args.transformer_ff = getattr(args, 'transformer_ff', 2048)\n",
    "args.heads = getattr(args, 'heads', 8)\n",
    "\n",
    "# Add transformer position parameters\n",
    "args.max_relative_positions = 0  # Default for standard transformer without relative positions\n",
    "args.position_encoding = True  # Enable position encoding\n",
    "args.param_init = 0.0  # Parameter initialization\n",
    "args.param_init_glorot = True  # Use Glorot initialization\n",
    "\n",
    "# Fix share_embeddings - set to False since we don't have shared vocabulary\n",
    "args.share_embeddings = False  # This was causing the assertion error\n",
    "args.share_decoder_embeddings = False  # Also disable this to be safe\n",
    "\n",
    "# Add training parameters needed by OpenNMT trainer\n",
    "args.truncated_decoder = 0  # Truncated BPTT\n",
    "args.max_generator_batches = getattr(args, 'max_generator_batches', 32)\n",
    "args.normalization = getattr(args, 'normalization', 'sents')\n",
    "args.accum_count = getattr(args, 'accum_count', 1)\n",
    "args.accum_steps = [0]\n",
    "args.average_decay = 0.0  # Exponential moving average decay\n",
    "args.average_every = 1  # Average every N updates\n",
    "args.report_manager = None\n",
    "args.valid_steps = getattr(args, 'valid_steps', 10000)\n",
    "args.early_stopping = 0\n",
    "args.early_stopping_criteria = None\n",
    "args.valid_batch_size = 32\n",
    "\n",
    "# Add the missing transformer attention parameters\n",
    "args.self_attn_type = \"scaled-dot\"  # Default self-attention type for transformer\n",
    "args.input_feed = 1  # Input feeding for RNN decoders\n",
    "args.copy_attn_type = None  # Type of copy attention\n",
    "args.generator_function = \"softmax\"  # Generator function\n",
    "\n",
    "# Add distributed training parameters\n",
    "args.local_rank = -1  # For distributed training (not used here)\n",
    "args.gpu_ranks = getattr(args, 'gpu_ranks', [0])  # List of GPUs to use\n",
    "args.gpu_verbose_level = 0  # GPU logging verbosity\n",
    "args.world_size = getattr(args, 'world_size', 1)  # Number of processes for distributed\n",
    "\n",
    "# Add other required parameters\n",
    "args.encoder_type = getattr(args, 'encoder_type', \"transformer\")\n",
    "args.decoder_type = getattr(args, 'decoder_type', \"transformer\") \n",
    "args.enc_layers = getattr(args, 'layers', 6)\n",
    "args.dec_layers = getattr(args, 'layers', 6)\n",
    "args.dropout = getattr(args, 'dropout', 0.1)\n",
    "args.attention_dropout = getattr(args, 'dropout', 0.1)\n",
    "args.bridge = \"\"\n",
    "args.aux_tune = False\n",
    "args.subword_prefix = \"‚ñÅ\"\n",
    "args.subword_prefix_is_joiner = False\n",
    "\n",
    "args.save_model = os.path.join(output_path, 'ckpt', 'model')\n",
    "args.log_file = os.path.join(output_path, 'log', 'log')\n",
    "args.tensorboard_log_dir = os.path.join(output_path, 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.563501Z",
     "iopub.status.idle": "2025-05-18T18:27:01.563780Z",
     "shell.execute_reply": "2025-05-18T18:27:01.563652Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.563640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary and dataset\n",
    "vocab = torch.load(data + '.vocab.pt')\n",
    "src_vocab = vocab['src'].fields[0][1].vocab.stoi\n",
    "tgt_vocab = vocab['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = BertKdDataset(data_db, bert_dump, \n",
    "                             src_vocab, tgt_vocab,\n",
    "                             max_len=150, k=args.kd_topk)\n",
    "\n",
    "# Create data loader\n",
    "BUCKET_SIZE = 8192\n",
    "train_sampler = TokenBucketSampler(\n",
    "    train_dataset.keys, BUCKET_SIZE, 6144,\n",
    "    batch_multiple=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=BertKdDataset.pad_collate)\n",
    "\n",
    "train_iter = cycle_loader(train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.564980Z",
     "iopub.status.idle": "2025-05-18T18:27:01.565293Z",
     "shell.execute_reply": "2025-05-18T18:27:01.565154Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.565141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from onmt.model_builder import build_model\n",
    "\n",
    "# Make sure nn is imported at the top of the notebook\n",
    "model = build_model(args, args, fields=vocab, checkpoint=None)\n",
    "model.to(device)\n",
    "\n",
    "# Build optimizer\n",
    "optim = Optimizer.from_opt(model, args, checkpoint=None)\n",
    "\n",
    "# Build model saver\n",
    "model_saver = build_model_saver(args, args, model, vocab, optim)\n",
    "\n",
    "# Build trainer\n",
    "trainer = build_trainer(args, 0, model, vocab, optim, model_saver=model_saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.566582Z",
     "iopub.status.idle": "2025-05-18T18:27:01.566931Z",
     "shell.execute_reply": "2025-05-18T18:27:01.566822Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.566808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# the problem is in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.568299Z",
     "iopub.status.idle": "2025-05-18T18:27:01.568621Z",
     "shell.execute_reply": "2025-05-18T18:27:01.568471Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.568456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train - for demonstration, we'll only do a few steps\n",
    "num_steps_to_run_kd = 100000  # Adjust for full training (paper used 100k steps)\n",
    "\n",
    "# Make sure the optimizer is tracking the step correctly\n",
    "if not hasattr(optim, 'training_step'):\n",
    "    optim._training_step = 0\n",
    "    \n",
    "# Define a custom iterator that provides batches without its own step limitation\n",
    "def manual_train_iter():\n",
    "    # We don't reset the optimizer step counter here anymore\n",
    "    global train_iter  # Use global instead of nonlocal for variables defined at module level\n",
    "    while True:  # This will keep yielding batches indefinitely\n",
    "        try:\n",
    "            batch = next(train_iter)\n",
    "        except StopIteration:\n",
    "            # Restart the iterator when we run out of batches\n",
    "            print(\"Restarting data iterator\")\n",
    "            train_iter = cycle_loader(train_loader, device)\n",
    "            batch = next(train_iter)\n",
    "        \n",
    "        # We let the trainer handle step counting now\n",
    "        yield batch\n",
    "\n",
    "print(\"Starting model training with knowledge distillation...\")\n",
    "# Now the trainer will properly control the number of steps\n",
    "trainer.train(\n",
    "    manual_train_iter(),\n",
    "    num_steps_to_run_kd,\n",
    "    save_checkpoint_steps=100,  # Save every 100 steps\n",
    "    valid_iter=None\n",
    ")\n",
    "\n",
    "print(f\"Model trained for {num_steps_to_run_kd} steps and saved to {output_path}/ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.569465Z",
     "iopub.status.idle": "2025-05-18T18:27:01.569794Z",
     "shell.execute_reply": "2025-05-18T18:27:01.569640Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.569626Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define paths for translation\n",
    "model_path = f\"{output_path}/ckpt/model_step_{num_steps_to_run_kd}.pt\"\n",
    "src_file = f\"{data_dir}/test.de.bert\"\n",
    "tgt_file = f\"{data_dir}/test.en.bert\"\n",
    "out_dir = \"output/translation\"\n",
    "ref_file = f\"{data_dir}/test.en\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Run translation if model exists\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model found at {model_path}. Running translation...\")\n",
    "    try:\n",
    "        # Run translation\n",
    "        !python opennmt/translate.py -model {model_path} \\\n",
    "                                    -src {src_file} \\\n",
    "                                    -tgt {tgt_file} \\\n",
    "                                    -output {out_dir}/result.en \\\n",
    "                                    -gpu 0 \\\n",
    "                                    -beam_size 5 -alpha 0.6 \\\n",
    "                                    -length_penalty wu\n",
    "\n",
    "        print(\"Translation completed. Detokenizing output...\")\n",
    "        # Check if translation output exists\n",
    "        if os.path.exists(f\"{out_dir}/result.en\"):\n",
    "            # Detokenize output\n",
    "            !python scripts/bert_detokenize.py --file {out_dir}/result.en \\\n",
    "                                          --output_dir {out_dir}\n",
    "\n",
    "            # Check if detokenized output exists\n",
    "            if os.path.exists(f\"{out_dir}/result.en.detok\"):\n",
    "                print(\"Evaluating with BLEU score...\")\n",
    "                # Evaluate with BLEU\n",
    "                !perl opennmt/tools/multi-bleu.perl {ref_file} \\\n",
    "                                               < {out_dir}/result.en.detok \\\n",
    "                                               > {out_dir}/result.bleu\n",
    "\n",
    "                # Display BLEU score if file exists\n",
    "                if os.path.exists(f\"{out_dir}/result.bleu\"):\n",
    "                    with open(f\"{out_dir}/result.bleu\", \"r\") as f:\n",
    "                        bleu_score = f.read().strip()\n",
    "                        print(f\"BLEU Score: {bleu_score}\")\n",
    "                else:\n",
    "                    print(\"Warning: BLEU score file was not generated. This might indicate an issue with the evaluation.\")\n",
    "            else:\n",
    "                print(\"Warning: Detokenized output file was not generated.\")\n",
    "        else:\n",
    "            print(\"Warning: Translation output file was not generated.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation process: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"Model file {model_path} not found. Skipping translation.\")\n",
    "    print(\"You need to train the model first or adjust the model path to point to an existing checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T18:27:01.571122Z",
     "iopub.status.idle": "2025-05-18T18:27:01.571385Z",
     "shell.execute_reply": "2025-05-18T18:27:01.571244Z",
     "shell.execute_reply.started": "2025-05-18T18:27:01.571235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the figures from the paper\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot CMLM finetuning\n",
    "axes[0].set_title('CMLM Finetuning')\n",
    "img = plt.imread('figures/cmlm-finetuning.png')\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot translation losses\n",
    "axes[1].set_title('Translation Losses')\n",
    "img = plt.imread('figures/translation-losses.png')\n",
    "axes[1].imshow(img)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Plot translation accuracy\n",
    "axes[2].set_title('Translation Accuracy')\n",
    "img = plt.imread('figures/translation-accuracy.png')\n",
    "axes[2].imshow(img)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
